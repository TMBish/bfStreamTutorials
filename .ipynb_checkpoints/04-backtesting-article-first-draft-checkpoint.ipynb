{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting With Historical Stream Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "Backtesting is the life-blood of most succesful wagering systems. In short it attempts to answer a single question for you:\n",
    "\n",
    "> $ \\tau $ : How much money will I win or lose if I started using this system to place bets with real money? \n",
    "\n",
    "Without a rigorous and quantitative backtesting approach it's really quite hard to estimate the answer to this question $ \\tau $ that will be even reliably on the right side of zero. \n",
    "\n",
    "You could live test your system with real bets at small stakes, however, this isn't the panacea it seems. It will take time (more than you think) for your results to converge to their long term expectation. How long? Answering this question will require some expertise with probability and statistics you might not have. Even more than that though is that depending on where you're betting your results at small stakes could be very different than at larger stakes. You might not be able get a good answer to $ \\tau $ until betting at full stakes at which point finding the answer might coincide with blowing up your gambling bankroll.\n",
    "\n",
    "Backtesting is also very hard. To perfectly backtest your own predicted probablility on a historical race or sporting match you need to produce 2 things:\n",
    "\n",
    "> (1) What would my predicted chance have been **exactly** for this selection in this market on this day in the past?\n",
    "\n",
    "> (2) What would have I decided to bet **at what odds (exactly)** and **for how much stake (exactly)** based on this prediction? \n",
    "\n",
    "> ** where the devil tends to be in those **exactly**s.\n",
    "\n",
    "The aim of the backtesting game is answering (2) as accurately as possible because it tells you exactly how much you would have made over your backtesting period, from there you can confidently project that rate of profitability forward. \n",
    "\n",
    "It's easy to make mistakes and small errors in the quantitative reasoning can lead you to extremely misguided projections downstream. \n",
    "\n",
    "Question (1) won't be in the scope of this notebook but it's equally (and probably more) important thant (2) but it is the key challenge of all predictive modelling exercises so there's plenty of discussion about it elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting on Betfair\n",
    "\n",
    "Answering quistion (2) for betting on the betfair exchange is difficult. The exhange is a dynamic system that changes from one micro second to the next.\n",
    "\n",
    "> What number should you use for odds? How much could you assume to get down at those odds?\n",
    "\n",
    "The conventional and easiest approach is to backtest at the BSP. The BSP is simple because it's a single number (to use for both back and lay bets) and is a taken price (there's no uncertainty about getting matched). Depending on the liquidity of the market a resonably sized stake might also not move the BSP very much. For some markets you may be able to safely assume you could be $10s of dollars at the BSP without moving it an inch. However, that's definitely not true of all BSP markets and you need to be generally aware that your betfair orders in the future **will** change the state of the exchange, and large bets **will** move the BSP in an unfavourable direction.\n",
    "\n",
    "Aside from uncertainty around the liquidity and resiliance of the BSP, many many markets don't have a BSP. So what do we do then?\n",
    "\n",
    "Typically what a lot of people (who have a relationship with betfair australia) do at this point is request a data dump. They might request an odds file for all australian harness race win markets since june 2018 with results and 4 different price points: the BSP, the last traded price, the weighted average price (WAP) traded in 3 minutes before the race starts, and the WAP for all bets before 3 mins before the race. \n",
    "\n",
    "However, you will likely need to be an existing VIP customer to get this file and it's not ideal: it might take 2 weeks to get, you can't refresh it, you can't test more hypothetical price points after your initial analysis amongst many other problems. \n",
    "\n",
    "> What if you could produce this valuable data file yourself?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betfair Stream Data\n",
    "\n",
    "Betfair's historical stream data is an extremely rich source of data. However, in it's raw form it's difficult to handle for the uninitiated. It also might not be immediately obvious how many different things this dataset could be used for without seeing some examples. These guides will hopefully demystify how to turn this raw data into a familiar and usable format whilst also hopefully providing some inpiration for the kinds of value that can be excavated from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Backtesting Betfair Hub Thoroughbred Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate using the stream files to backtest the outputs to a rating system we'll use the Australian Thoroughbred Rating model available on the Betfair Hub. The most recent model iteration only goes back till Feb 28th 2021 however as an illustrative example this is fine. We'd normally want to backtest with a lot more historical data than this.\n",
    "\n",
    "I'm interested to see how we would have fared betting all selections rated by this model according to a few different staking schemes and also at a few different price points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some python setup\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Scrape The Model Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you travel to the [betfair hub ratings page](https://www.betfair.com.au/hub/horse-racing-tips/) you'll find that URL links behind the ratings download buttons have a consistent URL pattern that looks very scrape friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/finding-hub-ratings-url.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take advantage of this consistency and use some simple python code to scrape all the ratings into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Function to return Pandas DF of hub ratings for a particular date\n",
    "def getHubRatings(dte):\n",
    "    \n",
    "    # Substitute the date into the URL\n",
    "    url = 'https://betfair-data-supplier-prod.herokuapp.com/api/widgets/kash-ratings-model/datasets?date={}presenter=RatingsPresenter&json=true'.format(dte)\n",
    "    \n",
    "    # Convert the response into JSON\n",
    "    responseJson = requests.get(url).json()\n",
    "        \n",
    "    hubList = []\n",
    "    \n",
    "    if not responseJson:\n",
    "        return(None)\n",
    "    \n",
    "    \n",
    "    # Want an normalised table (1 row per selection)\n",
    "    # Brute force / simple approach is to loop through meetings / races / runners and pull out the key fields\n",
    "    for meeting in responseJson['meetings']:\n",
    "        for race in meeting['races']:\n",
    "            for runner in race['runners']:\n",
    "                hubList.append(\n",
    "                    {\n",
    "                        'date': dte,\n",
    "                        'track': meeting['name'],\n",
    "                        'race_number': race['number'],\n",
    "                        'race_name': race['name'],\n",
    "                        'market_id': race['bfExchangeMarketId'],\n",
    "                        'selection_id':  runner['bfExchangeSelectionId'],\n",
    "                        'selection_name': runner['name'],\n",
    "                        'model_odds': runner['ratedPrice']\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "    out = pd.DataFrame(hubList)\n",
    "    \n",
    "#     for col in ['market_id', 'selection_id', 'track', 'race_name', 'selection_name']:\n",
    "#         out[col] = out[col].astype('str') \n",
    "                \n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      COWRA\n",
       "1      COWRA\n",
       "2      COWRA\n",
       "3      COWRA\n",
       "4      COWRA\n",
       "       ...  \n",
       "206     YORK\n",
       "207     YORK\n",
       "208     YORK\n",
       "209     YORK\n",
       "210     YORK\n",
       "Name: track, Length: 211, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the response from a single day\n",
    "x = getHubRatings(date(2021,3,1))\n",
    "x['track'] = x[\"track\"].astype('str')\n",
    "x['track']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-18 00:00:00\n",
      "2021-02-19 00:00:00\n",
      "2021-02-20 00:00:00\n",
      "2021-02-21 00:00:00\n",
      "2021-02-22 00:00:00\n",
      "2021-02-23 00:00:00\n",
      "2021-02-24 00:00:00\n",
      "2021-02-25 00:00:00\n",
      "2021-02-26 00:00:00\n",
      "2021-02-27 00:00:00\n",
      "2021-02-28 00:00:00\n",
      "2021-03-01 00:00:00\n",
      "2021-03-02 00:00:00\n",
      "2021-03-03 00:00:00\n",
      "2021-03-04 00:00:00\n",
      "2021-03-05 00:00:00\n",
      "2021-03-06 00:00:00\n",
      "2021-03-07 00:00:00\n",
      "2021-03-08 00:00:00\n",
      "2021-03-09 00:00:00\n",
      "2021-03-10 00:00:00\n",
      "2021-03-11 00:00:00\n",
      "2021-03-12 00:00:00\n",
      "2021-03-13 00:00:00\n",
      "2021-03-14 00:00:00\n",
      "2021-03-15 00:00:00\n",
      "2021-03-16 00:00:00\n",
      "2021-03-17 00:00:00\n",
      "2021-03-18 00:00:00\n",
      "2021-03-19 00:00:00\n",
      "2021-03-20 00:00:00\n",
      "2021-03-21 00:00:00\n",
      "2021-03-22 00:00:00\n",
      "2021-03-23 00:00:00\n",
      "2021-03-24 00:00:00\n",
      "2021-03-25 00:00:00\n",
      "2021-03-26 00:00:00\n",
      "2021-03-27 00:00:00\n",
      "2021-03-28 00:00:00\n",
      "2021-03-29 00:00:00\n",
      "2021-03-30 00:00:00\n",
      "2021-03-31 00:00:00\n",
      "2021-04-01 00:00:00\n",
      "2021-04-02 00:00:00\n",
      "2021-04-03 00:00:00\n",
      "2021-04-04 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Loop through all recent history\n",
    "dateDFList = []\n",
    "dateList = pd.date_range(date(2021,2,18),date.today()-timedelta(days=1),freq='d')\n",
    "\n",
    "for dte in dateList:\n",
    "    print(dte)\n",
    "    dateDFList.append(getHubRatings(dte))\n",
    "    \n",
    "# Concatenate (add rows to rows) all the dataframes within the list\n",
    "hubRatings = pd.concat(dateDFList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>track</th>\n",
       "      <th>race_number</th>\n",
       "      <th>race_name</th>\n",
       "      <th>market_id</th>\n",
       "      <th>selection_id</th>\n",
       "      <th>selection_name</th>\n",
       "      <th>model_odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>DOOMBEN</td>\n",
       "      <td>1</td>\n",
       "      <td>R1 1200m 3yo</td>\n",
       "      <td>1.179418181</td>\n",
       "      <td>38523320</td>\n",
       "      <td>11. Vast Kama</td>\n",
       "      <td>34.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>DOOMBEN</td>\n",
       "      <td>1</td>\n",
       "      <td>R1 1200m 3yo</td>\n",
       "      <td>1.179418181</td>\n",
       "      <td>38523319</td>\n",
       "      <td>10. Triptonic</td>\n",
       "      <td>21.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>DOOMBEN</td>\n",
       "      <td>1</td>\n",
       "      <td>R1 1200m 3yo</td>\n",
       "      <td>1.179418181</td>\n",
       "      <td>35773035</td>\n",
       "      <td>9. Right Reason</td>\n",
       "      <td>10.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>DOOMBEN</td>\n",
       "      <td>1</td>\n",
       "      <td>R1 1200m 3yo</td>\n",
       "      <td>1.179418181</td>\n",
       "      <td>38523318</td>\n",
       "      <td>8. Off Road</td>\n",
       "      <td>40.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>DOOMBEN</td>\n",
       "      <td>1</td>\n",
       "      <td>R1 1200m 3yo</td>\n",
       "      <td>1.179418181</td>\n",
       "      <td>38523317</td>\n",
       "      <td>7. More Than Value</td>\n",
       "      <td>77.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>2021-04-04</td>\n",
       "      <td>SUNSHINE COAST</td>\n",
       "      <td>8</td>\n",
       "      <td>R8 1000m Hcap</td>\n",
       "      <td>1.181395103</td>\n",
       "      <td>23082987</td>\n",
       "      <td>5. Arrow Express</td>\n",
       "      <td>56.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>2021-04-04</td>\n",
       "      <td>SUNSHINE COAST</td>\n",
       "      <td>8</td>\n",
       "      <td>R8 1000m Hcap</td>\n",
       "      <td>1.181395103</td>\n",
       "      <td>39000338</td>\n",
       "      <td>6. The Diamond</td>\n",
       "      <td>8.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>2021-04-04</td>\n",
       "      <td>SUNSHINE COAST</td>\n",
       "      <td>8</td>\n",
       "      <td>R8 1000m Hcap</td>\n",
       "      <td>1.181395103</td>\n",
       "      <td>22773079</td>\n",
       "      <td>7. Lytafyre</td>\n",
       "      <td>75.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>2021-04-04</td>\n",
       "      <td>SUNSHINE COAST</td>\n",
       "      <td>8</td>\n",
       "      <td>R8 1000m Hcap</td>\n",
       "      <td>1.181395103</td>\n",
       "      <td>4350201</td>\n",
       "      <td>8. Super Freak</td>\n",
       "      <td>18.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>2021-04-04</td>\n",
       "      <td>SUNSHINE COAST</td>\n",
       "      <td>8</td>\n",
       "      <td>R8 1000m Hcap</td>\n",
       "      <td>1.181395103</td>\n",
       "      <td>27750351</td>\n",
       "      <td>9. Haunted House</td>\n",
       "      <td>46.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25293 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date           track  race_number      race_name    market_id  \\\n",
       "0   2021-02-18         DOOMBEN            1   R1 1200m 3yo  1.179418181   \n",
       "1   2021-02-18         DOOMBEN            1   R1 1200m 3yo  1.179418181   \n",
       "2   2021-02-18         DOOMBEN            1   R1 1200m 3yo  1.179418181   \n",
       "3   2021-02-18         DOOMBEN            1   R1 1200m 3yo  1.179418181   \n",
       "4   2021-02-18         DOOMBEN            1   R1 1200m 3yo  1.179418181   \n",
       "..         ...             ...          ...            ...          ...   \n",
       "581 2021-04-04  SUNSHINE COAST            8  R8 1000m Hcap  1.181395103   \n",
       "582 2021-04-04  SUNSHINE COAST            8  R8 1000m Hcap  1.181395103   \n",
       "583 2021-04-04  SUNSHINE COAST            8  R8 1000m Hcap  1.181395103   \n",
       "584 2021-04-04  SUNSHINE COAST            8  R8 1000m Hcap  1.181395103   \n",
       "585 2021-04-04  SUNSHINE COAST            8  R8 1000m Hcap  1.181395103   \n",
       "\n",
       "    selection_id      selection_name  model_odds  \n",
       "0       38523320       11. Vast Kama       34.28  \n",
       "1       38523319       10. Triptonic       21.22  \n",
       "2       35773035     9. Right Reason       10.23  \n",
       "3       38523318         8. Off Road       40.75  \n",
       "4       38523317  7. More Than Value       77.49  \n",
       "..           ...                 ...         ...  \n",
       "581     23082987    5. Arrow Express       56.83  \n",
       "582     39000338      6. The Diamond        8.54  \n",
       "583     22773079         7. Lytafyre       75.57  \n",
       "584      4350201      8. Super Freak       18.85  \n",
       "585     27750351    9. Haunted House       46.98  \n",
       "\n",
       "[25293 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hubRatings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Assembling and Odds File\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So part 1 was very painless. This is how we like data: served by some API or available in a nice tabular format on a webpage ready to be scraped with standard tools in available in popular languages.\n",
    "\n",
    "Unfortunately, it won't be so painless to assemble our odds file. We'll find out why it's tricky as we go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll be using is the historical exchange data available from (this website)[https://historicdata.betfair.com/#/home]. The data available through this service is streaming JSON data. There are a few options available relating to granularity (how many time points per second the data updates at). \n",
    "\n",
    "Essentially what the data allows us to do is, for a particular market, recreate the exact state of the betfair exchange say at 150 microseconds before the market closed. What we mean when we say the **state of the exchange** we mean two things a) what are the current open orders in the exchange b) what are the current traded volumes on each selection at each price point. \n",
    "\n",
    "With these 2 pieces of information we can build a rich view of the dynamics of exchange and also build out all of the summary metrics (WAP etc) we might have previously needed betfair to help with.\n",
    "\n",
    "Now, for our purposes 50 miscro-second intervaled data is huge overkill but you could imagine needing this kind of granularity for other kinds of wagering systems - for example a high frequency trading algorithm of some sort that needed to made many decisions and actions every second. \n",
    "\n",
    "Let's take a look at what the data looks like for a single market:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/stream-data-example.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So look pretty intractable. For this particular market there's 14,384 lines which each consistent of a json packet. If you're not a data engineer (neither am I) your head might explode thinking about how you could read this into your computer and transform it into something usable.\n",
    "\n",
    "The data looks like this because it is saved from a special betfair API called the Stream API which which is used by high end betfair API users and which delivers fast speeds other performance improvements over the normal \"polling\" API.\n",
    "\n",
    "Now what's good about that, for the purposes of our exercise, is that the very nice python package `betfairlightweight` has the functionality built to not only parse the Stream API live but also these historical versions of the stream data. Without it we'd be *very* far away from the finish line, with `betfairlightweight` we're pretty close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalising The Data\n",
    "\n",
    "There's a few elements to handling these files so I'll step through the 4 key components conceptually. \n",
    "\n",
    "Because these files are so large and unprocessed it won't look the same as your normal data read in python: read file (csv, json, text etc.) into memory and use python functions to process into usable format.\n",
    "\n",
    "1. Load The Archives\n",
    "2. Loop through Markets\n",
    "3. Loop through Market States\n",
    "4. Process Market States\n",
    "5. Write Processed Values To File\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAR Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing is these files come as tar archive files which special kind of file that we'll need to unpack. Thankfully we can do that with python too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading from tar and extracting files\n",
    "def load_markets(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        print(file_path)\n",
    "        if os.path.isdir(file_path):\n",
    "            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):\n",
    "                f = bz2.BZ2File(path, 'rb')\n",
    "                yield f\n",
    "                f.close()\n",
    "        elif os.path.isfile(file_path):\n",
    "            ext = os.path.splitext(file_path)[1]\n",
    "            # iterate through a tar archive\n",
    "            if ext == '.tar':\n",
    "                with tarfile.TarFile(file_path) as archive:\n",
    "                    for file in archive:\n",
    "                        yield bz2.open(archive.extractfile(file))\n",
    "            # or a zip archive\n",
    "            elif ext == '.zip':\n",
    "                with zipfile.ZipFile(file_path) as archive:\n",
    "                    for file in archive.namelist():\n",
    "                        yield bz2.open(archive.open(file))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/2021_02_FebRacingPro.tar\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import betfairlightweight\n",
    "from betfairlightweight import StreamListener\n",
    "import logging\n",
    "import requests\n",
    "import tarfile\n",
    "import bz2\n",
    "from unittest.mock import patch\n",
    "\n",
    "import logging\n",
    "from typing import List, Set, Dict, Tuple, Optional\n",
    "\n",
    "from unittest.mock import patch\n",
    "from itertools import zip_longest\n",
    "import functools\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import zipfile\n",
    "import bz2\n",
    "import glob\n",
    "\n",
    "# importing data types\n",
    "import betfairlightweight\n",
    "from betfairlightweight.resources.bettingresources import (\n",
    "    PriceSize,\n",
    "    MarketBook\n",
    ")\n",
    "\n",
    "\n",
    "data_path = [\n",
    "#     \"./data/2021_01_JanRacingPro.tar\",\n",
    "    \"./data/2021_02_FebRacingPro.tar\"\n",
    "]\n",
    "\n",
    "# Betfair Lightweight Boilerplate\n",
    "\n",
    "# create trading instance (don't need username/password)\n",
    "trading = betfairlightweight.APIClient(\"username\", \"password\")\n",
    "\n",
    "# create listener\n",
    "listener = StreamListener(max_latency=None)\n",
    "\n",
    "# rounding to 2 decimal places or returning '' if blank\n",
    "def as_str(v: float) -> str:\n",
    "    return '%.2f' % v if v is not None else ''\n",
    "\n",
    "# splitting race name and returning the parts \n",
    "def split_anz_horse_market_name(market_name: str) -> (str, str, str):\n",
    "    # return race no, length, race type\n",
    "    # input sample: R6 1400m Grp1\n",
    "    parts = market_name.split(' ')\n",
    "    race_no = parts[0] # return example R6\n",
    "    race_len = parts[1] # return example 1400m\n",
    "    race_type = parts[2].lower() # return example grp1, trot, pace\n",
    "\n",
    "    return (race_no, race_len, race_type)\n",
    "\n",
    "# filtering markets to those that fit the following criteria\n",
    "def filter_market(market: MarketBook) -> bool: \n",
    "    d = market.market_definition\n",
    "    return (d.country_code == 'AU' \n",
    "        and d.market_type == 'WIN' \n",
    "        and (c := split_anz_horse_market_name(d.name)[2]) != 'trot' and c != 'pace')\n",
    "\n",
    "# record prices to a file\n",
    "with open(\"outputs/tho-odds-feb.csv\", \"w+\") as output:\n",
    "    # defining column headers\\\n",
    "    \n",
    "    # Column Headers\n",
    "    output.write(\"market_id,event_date,country,track,market_name,selection_id,selection_name,result,bsp,matched_volume, best_back_1m, best_back_5m \\n\")\n",
    "\n",
    "    for file_obj in load_markets(data_path):\n",
    "\n",
    "        # Instantiate a \"stream\" object\n",
    "        stream = trading.streaming.create_historical_generator_stream(\n",
    "            file_path=file_obj,\n",
    "            listener=listener,\n",
    "        )\n",
    "\n",
    "\n",
    "        # For this stream object execute the following Lambda function\n",
    "        with patch(\"builtins.open\", lambda f, _: f): \n",
    "\n",
    "            evaluate_market = False\n",
    "            preplay_market = None\n",
    "            postplay_market = None\n",
    "            preplay_traded = None\n",
    "            postplay_traded = None\n",
    "            t5m = False\n",
    "            t1m = False\n",
    "\n",
    "            gen = stream.get_generator()\n",
    "            for market_books in gen():\n",
    "                for market_book in market_books:\n",
    "\n",
    "                    # skipping markets that don't meet the filter\n",
    "                    if evaluate_market == False and filter_market(market_book) == False:\n",
    "                        continue\n",
    "                    else:\n",
    "                        evaluate_market = True\n",
    "\n",
    "                    # final market view before market goes in play\n",
    "                    if preplay_market is not None and preplay_market.inplay != market_book.inplay:\n",
    "                        preplay_traded = [ (r.last_price_traded, r.ex.traded_volume.copy()) for r in preplay_market.runners ]\n",
    "                    preplay_market = market_book\n",
    "\n",
    "                    # final market view at the conclusion of the market\n",
    "                    if postplay_market is not None and postplay_market.status == \"OPEN\" and market_book.status != postplay_market.status:\n",
    "                        postplay_traded = [ (r.last_price_traded, r.ex.traded_volume.copy()) for r in market_book.runners ]\n",
    "                    postplay_market = market_book   \n",
    "                    \n",
    "                    seconds_to_start = (\n",
    "                        market_book.market_definition.market_time - market_book.publish_time\n",
    "                    ).total_seconds()\n",
    "                    \n",
    "                    # Best Available To Back 5m\n",
    "                    if not t5m:\n",
    "                        if seconds_to_start < 5*60:\n",
    "                            t5m_market = market_book\n",
    "                            t5m = True\n",
    "                            \n",
    "                    # Best Available To Back 1m\n",
    "                    if not t1m:\n",
    "                        if seconds_to_start < 1*60:\n",
    "                            t1m_market = market_book\n",
    "                            t1m = True\n",
    "                    \n",
    "            # no price data for market\n",
    "            if postplay_traded is None:\n",
    "                continue; \n",
    "\n",
    "            # Runner Metadata\n",
    "            runner_data = [\n",
    "                {\n",
    "                    'selection_id': r.selection_id,\n",
    "                    'selection_name': next((rd.name for rd in postplay_market.market_definition.runners if rd.selection_id == r.selection_id), None),\n",
    "                    'selection_status': r.status,\n",
    "                    'sp': r.sp.actual_sp\n",
    "                }\n",
    "                for r in postplay_market.runners \n",
    "            ]\n",
    "            \n",
    "            # Total Matched Volume  \n",
    "            # _____________________\n",
    "            \n",
    "            def ladder_traded_volume(ladder):\n",
    "                return(sum([rung.size for rung in ladder]))\n",
    "\n",
    "            selection_traded_volume = [ ladder_traded_volume(runner[1]) for runner in postplay_traded ]\n",
    "\n",
    "            \n",
    "            # Best Available To Back\n",
    "            # ______________________\n",
    "            \n",
    "            def best_back(availableLadder):\n",
    "                if len(availableLadder) == 0:\n",
    "                    return(None)\n",
    "                else:\n",
    "                    return(availableLadder[0].price)\n",
    "\n",
    "            bestBack5m = [ best_back(runner.ex.available_to_back) for runner in t5m_market.runners]\n",
    "\n",
    "            bestBack1m = [ best_back(runner.ex.available_to_back) for runner in t1m_market.runners]\n",
    "            \n",
    "            # Writing To CSV\n",
    "            # ______________________\n",
    "            \n",
    "            for (runnerMeta, runnerTradedVolume, bb5m, bb1m) in zip(runner_data, selection_traded_volume, bestBack5m, bestBack1m):\n",
    "                \n",
    "                if runnerMeta['selection_status'] != 'REMOVED':\n",
    "                \n",
    "                    output.write(\n",
    "                        \"{},{},{},{},{},{},{},{},{},{},{},{} \\n\".format(\n",
    "                            postplay_market.market_id,\n",
    "                            postplay_market.market_definition.market_time,\n",
    "                            postplay_market.market_definition.country_code,\n",
    "                            postplay_market.market_definition.venue,\n",
    "                            postplay_market.market_definition.name,\n",
    "                            runnerMeta['selection_id'],\n",
    "                            runnerMeta['selection_name'],\n",
    "                            runnerMeta['selection_status'],\n",
    "                            runnerMeta['sp'],\n",
    "                            runnerTradedVolume,\n",
    "                            bb5m,\n",
    "                            bb1m\n",
    "                        )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Backtesting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_id</th>\n",
       "      <th>event_date</th>\n",
       "      <th>country</th>\n",
       "      <th>track</th>\n",
       "      <th>market_name</th>\n",
       "      <th>selection_id</th>\n",
       "      <th>selection_name</th>\n",
       "      <th>result</th>\n",
       "      <th>bsp</th>\n",
       "      <th>matched_volume</th>\n",
       "      <th>best_back_1m</th>\n",
       "      <th>best_back_5m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.178629</td>\n",
       "      <td>2021-02-01 02:30:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>Donald</td>\n",
       "      <td>R1 1000m Mdn</td>\n",
       "      <td>37829855</td>\n",
       "      <td>1. Bartholdi Boy</td>\n",
       "      <td>LOSER</td>\n",
       "      <td>5.28</td>\n",
       "      <td>18712.51</td>\n",
       "      <td>5.4</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.178629</td>\n",
       "      <td>2021-02-01 02:30:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>Donald</td>\n",
       "      <td>R1 1000m Mdn</td>\n",
       "      <td>38352252</td>\n",
       "      <td>2. Ians Boy</td>\n",
       "      <td>LOSER</td>\n",
       "      <td>14.37</td>\n",
       "      <td>3229.95</td>\n",
       "      <td>20</td>\n",
       "      <td>15.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.178629</td>\n",
       "      <td>2021-02-01 02:30:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>Donald</td>\n",
       "      <td>R1 1000m Mdn</td>\n",
       "      <td>28230647</td>\n",
       "      <td>3. Kumana</td>\n",
       "      <td>LOSER</td>\n",
       "      <td>37.36</td>\n",
       "      <td>1927.57</td>\n",
       "      <td>44</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.178629</td>\n",
       "      <td>2021-02-01 02:30:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>Donald</td>\n",
       "      <td>R1 1000m Mdn</td>\n",
       "      <td>38352253</td>\n",
       "      <td>4. Blak Hart</td>\n",
       "      <td>LOSER</td>\n",
       "      <td>6.39</td>\n",
       "      <td>11473.31</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.178629</td>\n",
       "      <td>2021-02-01 02:30:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>Donald</td>\n",
       "      <td>R1 1000m Mdn</td>\n",
       "      <td>38352254</td>\n",
       "      <td>5. Fleet Dreams</td>\n",
       "      <td>LOSER</td>\n",
       "      <td>23.68</td>\n",
       "      <td>2847.36</td>\n",
       "      <td>38</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12430</th>\n",
       "      <td>1.179746</td>\n",
       "      <td>2021-02-26 11:00:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>Moonee Valley</td>\n",
       "      <td>R8 1200m Hcap</td>\n",
       "      <td>13321221</td>\n",
       "      <td>9. Queen Annabel</td>\n",
       "      <td>LOSER</td>\n",
       "      <td>52.78</td>\n",
       "      <td>4469.37</td>\n",
       "      <td>40</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12431</th>\n",
       "      <td>1.179746</td>\n",
       "      <td>2021-02-26 11:00:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>Moonee Valley</td>\n",
       "      <td>R8 1200m Hcap</td>\n",
       "      <td>22732857</td>\n",
       "      <td>10. Madam Superior</td>\n",
       "      <td>LOSER</td>\n",
       "      <td>11.54</td>\n",
       "      <td>22415.34</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12432</th>\n",
       "      <td>1.179746</td>\n",
       "      <td>2021-02-26 11:00:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>Moonee Valley</td>\n",
       "      <td>R8 1200m Hcap</td>\n",
       "      <td>38607948</td>\n",
       "      <td>11. Ping Ping</td>\n",
       "      <td>LOSER</td>\n",
       "      <td>232.91</td>\n",
       "      <td>1072.85</td>\n",
       "      <td>310</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12433</th>\n",
       "      <td>1.179746</td>\n",
       "      <td>2021-02-26 11:00:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>Moonee Valley</td>\n",
       "      <td>R8 1200m Hcap</td>\n",
       "      <td>19167308</td>\n",
       "      <td>12. Onslaught</td>\n",
       "      <td>LOSER</td>\n",
       "      <td>55.00</td>\n",
       "      <td>4006.70</td>\n",
       "      <td>75</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12434</th>\n",
       "      <td>1.179746</td>\n",
       "      <td>2021-02-26 11:00:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>Moonee Valley</td>\n",
       "      <td>R8 1200m Hcap</td>\n",
       "      <td>38607949</td>\n",
       "      <td>13. Lucifers Reward</td>\n",
       "      <td>LOSER</td>\n",
       "      <td>45.85</td>\n",
       "      <td>7288.89</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12435 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       market_id           event_date country          track    market_name  \\\n",
       "0       1.178629  2021-02-01 02:30:00      AU         Donald   R1 1000m Mdn   \n",
       "1       1.178629  2021-02-01 02:30:00      AU         Donald   R1 1000m Mdn   \n",
       "2       1.178629  2021-02-01 02:30:00      AU         Donald   R1 1000m Mdn   \n",
       "3       1.178629  2021-02-01 02:30:00      AU         Donald   R1 1000m Mdn   \n",
       "4       1.178629  2021-02-01 02:30:00      AU         Donald   R1 1000m Mdn   \n",
       "...          ...                  ...     ...            ...            ...   \n",
       "12430   1.179746  2021-02-26 11:00:00      AU  Moonee Valley  R8 1200m Hcap   \n",
       "12431   1.179746  2021-02-26 11:00:00      AU  Moonee Valley  R8 1200m Hcap   \n",
       "12432   1.179746  2021-02-26 11:00:00      AU  Moonee Valley  R8 1200m Hcap   \n",
       "12433   1.179746  2021-02-26 11:00:00      AU  Moonee Valley  R8 1200m Hcap   \n",
       "12434   1.179746  2021-02-26 11:00:00      AU  Moonee Valley  R8 1200m Hcap   \n",
       "\n",
       "       selection_id       selection_name result     bsp  matched_volume  \\\n",
       "0          37829855     1. Bartholdi Boy  LOSER    5.28        18712.51   \n",
       "1          38352252          2. Ians Boy  LOSER   14.37         3229.95   \n",
       "2          28230647            3. Kumana  LOSER   37.36         1927.57   \n",
       "3          38352253         4. Blak Hart  LOSER    6.39        11473.31   \n",
       "4          38352254      5. Fleet Dreams  LOSER   23.68         2847.36   \n",
       "...             ...                  ...    ...     ...             ...   \n",
       "12430      13321221     9. Queen Annabel  LOSER   52.78         4469.37   \n",
       "12431      22732857   10. Madam Superior  LOSER   11.54        22415.34   \n",
       "12432      38607948        11. Ping Ping  LOSER  232.91         1072.85   \n",
       "12433      19167308        12. Onslaught  LOSER   55.00         4006.70   \n",
       "12434      38607949  13. Lucifers Reward  LOSER   45.85         7288.89   \n",
       "\n",
       "       best_back_1m  best_back_5m   \n",
       "0               5.4           4.9   \n",
       "1                20          15.5   \n",
       "2                44            46   \n",
       "3               5.7           5.9   \n",
       "4                38            27   \n",
       "...             ...            ...  \n",
       "12430            40            44   \n",
       "12431            12            13   \n",
       "12432           310           180   \n",
       "12433            75            46   \n",
       "12434            30            29   \n",
       "\n",
       "[12435 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in odds file\n",
    "bfOdds = pd.read_csv(\"outputs/tho-odds-feb.csv\")\n",
    "\n",
    "bfOdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date              datetime64[ns]\n",
       "track                     object\n",
       "race_number                int64\n",
       "race_name                 object\n",
       "market_id                 object\n",
       "selection_id              object\n",
       "selection_name            object\n",
       "model_odds               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hubRatings.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.178629\n",
       "1        1.178629\n",
       "2        1.178629\n",
       "3        1.178629\n",
       "4        1.178629\n",
       "           ...   \n",
       "12430    1.179746\n",
       "12431    1.179746\n",
       "12432    1.179746\n",
       "12433    1.179746\n",
       "12434    1.179746\n",
       "Name: market_id, Length: 12435, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfOdds[\"market_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and float64 columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-67f1cafed530>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Join Ratings And Odds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhubRatings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbfOdds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'market_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'selection_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#hubRatings.join(bfOdds, on = ['market_id', 'selection_id'], how = 'left')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m ) -> \"DataFrame\":\n\u001b[1;32m---> 74\u001b[1;33m     op = _MergeOperation(\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m         \u001b[1;31m# to avoid incompatible dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 656\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m         \u001b[1;31m# If argument passed to validate,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1163\u001b[0m                     \u001b[0minferred_right\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minferred_left\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m                 ):\n\u001b[1;32m-> 1165\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m             \u001b[1;31m# datetimelikes must match exactly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on object and float64 columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "# Join Ratings And Odds\n",
    "\n",
    "pd.merge(hubRatings, bfOdds, on = ['market_id', 'selection_id'])\n",
    "\n",
    "#hubRatings.join(bfOdds, on = ['market_id', 'selection_id'], how = 'left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
